	Градиентный спуск - база 

![[Pasted image 20250210231800.png]]

	
	
	
	Adagrad (adaptive gradient descent) для каждого веса своё изменение градиентного шага 
т.к learning rate с каждой итерацией уменьшатся, то со временем веса будут уменьшаться всё меньше и меньше и может случиться так, что какой то вес w_i нужно стало обновить только под конец, когда learning rate уже маленький. Для решения решили сделать учёт прошлых увеличений 
![[Pasted image 20250204093952.png]]



	Momentum (с инерцией)
Двигается по инерции, то есть учитывает предыдущие направления
 ![[Pasted image 20250204092801.png]]
 
	
	SGD - стохастический градиентный спуск - база кормит

	SAG - stochastic average gradient descent - (на каждой итерации считается только одна производная и усредняется с остальными)