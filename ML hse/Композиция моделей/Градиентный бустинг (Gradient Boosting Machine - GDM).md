![[Pasted image 20250206110653.png]]
![[Pasted image 20250206111221.png]]
# То есть здесь мы на первом шаге берём b1 с коэф $\gamma$ = 1, обучаем на MSE, далее берём b2, которое пытаемся приблизить к $s_i = y_i-b_1(x_i)$. ((Минимизируем MSE для b2 и $s_i$. Т.е мы должны сделать b2 приблизительно равным $s_i$))
![[Pasted image 20250206111232.png]]
![[Pasted image 20250206111302.png]]
![[Pasted image 20250206111311.png]]
![[Pasted image 20250206111324.png]]


# #ГрадиентныйБустингКакРаботает
# Мы хотим построить композицию моделей $a_N(x)$, которая даст наилучший результат.
- Сначала обучаем первую базовую модель $b_1(x)$
- Далее индуктивно говорим: мы обучили $b_1(x)..b_{N-1}(x)$ модели, $a_{N-1} = \sum_{n-1}^{N-1} b_n(x)$, и после этого, чтобы получить $a_N$ мы минимизируем функционал потерь с помощью какой то модели $b_N$, которую получаем, из условия минимизации функционала потерь $L(y_i, a_{N-1}(x_i))$ с помощью взятия производной от него по переменной z: $L(y_i,z)$. Взятие производной означает получение информации о том, куда нужно шагнуть. Далее пытаемся аппроксимировать какую то функцию $b_N(x)$ к этому шагу $s_i$ (т.е мы шагаем не прям куда нужно а с помощью какой то аппроксимированной модели, то есть более шумно). 
- Далее (картинка 1) мы подбираем такую новую 

![[Pasted image 20250206112659.png]]

# Картинка 1

![[Pasted image 20250206112729.png]]
![[Pasted image 20250206112804.png]]
![[Pasted image 20250206112814.png]]
![[Pasted image 20250206112822.png]]




# Пример
![[Pasted image 20250206163929.png]]
![[Pasted image 20250206164002.png]]
![[Pasted image 20250206163948.png]]



# Регрессия - т.е для регрессии с MSE мы будем двигать каждый раз нашу композицию на разницу между ответом на N-1 модели и $y_i$.
![[Pasted image 20250206165702.png]]
# Классификация
![[Pasted image 20250206165545.png]]
![[Pasted image 20250206165627.png]]
