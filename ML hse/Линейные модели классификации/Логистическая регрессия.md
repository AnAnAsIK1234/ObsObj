### То есть здесь сначала рассматриваем n одинаковых объектов, у которых разные yi. Тогда логично было бы для каждого выдавать $b(x_i)$  = $b = const$, равной частоте положительных yi. Далее при устремлении n к бесконечности справа у нас получится вероятность того, что данный объект является положительным классом. А Слева (2 рис) получится матожидание функции ошибки для данного объекта


# (первый синий овал:  мы сами захотели чтобы в процессе минимизации функционала ошибки мы пришли к вероятности. Т.е мы уменьшаем так, чтобы в конце концов наша модель b выдавала вероятности положительного класса)

(синий овал - то, что мы хотим:  подобрать такие b, чтобы )
![[Pasted image 20250204162107.png]]
![[Pasted image 20250204162132.png]]
![[Pasted image 20250204162157.png]]
![[Pasted image 20250204162209.png]]

# #BinaryCrossEntropy - выводится из предположения, что у нас всего два варианта для с.в(т.е либо положительный класс, либо отрицательный), далее запишем функцию правдоподобия для этой случайной величины и далее логарифмируем как обычно $\Rightarrow$ получаем ==BinaryCrossEntropy==  
![[Pasted image 20250204164113.png]]
![[Pasted image 20250204164127.png]]
![[Pasted image 20250204164240.png]]
![[Pasted image 20250204164542.png]]
	